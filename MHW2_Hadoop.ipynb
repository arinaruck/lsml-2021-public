{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Мини задание 2\n",
    "\n",
    "К заданию прилагается [дополнительный ноутбук с важными заметками](https://github.com/ADKosm/lsml-2021-public/blob/main/MHW%202.%20Hadoop.%20Additional%20notes.ipynb). Прочтите его перед выполнением домашней работы.\n",
    "\n",
    "В этой домашней работе будет два подзадания.\n",
    "\n",
    "Мы продолжает работать с датасетом из Авито - https://www.kaggle.com/c/avito-context-ad-clicks ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arinaruck/Desktop/cs/lsml/lsml-2021-public\n",
      "/Users/arinaruck/Desktop/cs/lsml/lsml-2021-public\r\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/arinaruck/Desktop/cs/lsml/lsml-2021-public/\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb   \u001b[34mlsml-2021-public\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"fa34e92c-e581-439a-b6ae-b41b61270513\",\n",
      "    \"id\": \"dfeaef04-2e5c-4480-8ebf-301863ed526a\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [],\n",
      "    \"name\": \"Microsoft Azure Sponsorship 2\",\n",
      "    \"state\": \"Disabled\",\n",
      "    \"tenantId\": \"fa34e92c-e581-439a-b6ae-b41b61270513\",\n",
      "    \"user\": {\n",
      "      \"name\": \"arina.ruck@gmail.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"fa34e92c-e581-439a-b6ae-b41b61270513\",\n",
      "    \"id\": \"3750c13c-7871-4812-9fd5-94fb4ce86681\",\n",
      "    \"isDefault\": false,\n",
      "    \"managedByTenants\": [],\n",
      "    \"name\": \"Lab1 21 Rak Arina Sergeevna\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"fa34e92c-e581-439a-b6ae-b41b61270513\",\n",
      "    \"user\": {\n",
      "      \"name\": \"arina.ruck@gmail.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! az login -u arina.ruck@gmail.com -p $(cat password-file.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arinaruck/Desktop/cs/lsml/lsml-2021-public/hadoop\n"
     ]
    }
   ],
   "source": [
    "!rm -r hadoop\n",
    "! mkdir -p hadoop\n",
    "\n",
    "%cd hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop.tf\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop.tf\n",
    "\n",
    "resource \"azurerm_storage_container\" \"lsml_hadoop_sc\" {\n",
    "  name                  = \"hw2-lslm-storage-container\"\n",
    "  storage_account_name  = azurerm_storage_account.lsml_sa.name\n",
    "  container_access_type = \"private\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" {\n",
    "  name                = \"hw2-lsml-hdicluster\"\n",
    "  resource_group_name = azurerm_resource_group.lsml_rg.name\n",
    "  location            = azurerm_resource_group.lsml_rg.location\n",
    "  cluster_version     = \"4.0\"\n",
    "  tier                = \"Standard\"\n",
    "\n",
    "  component_version {\n",
    "    hadoop = \"3.1\"\n",
    "  }\n",
    "\n",
    "  gateway {\n",
    "    enabled  = true\n",
    "    username = \"azureuser\"\n",
    "    password = \"Password123!\"\n",
    "  }\n",
    "\n",
    "  storage_account {\n",
    "    storage_container_id = azurerm_storage_container.lsml_hadoop_sc.id\n",
    "    storage_account_key  = azurerm_storage_account.lsml_sa.primary_access_key\n",
    "    is_default           = true\n",
    "  }\n",
    "\n",
    "  roles {\n",
    "    head_node {\n",
    "      vm_size  = \"A5\"  # 2 cpu 4 ram\n",
    "      username = \"azureuser\"\n",
    "      password = \"Password123!\"\n",
    "    }\n",
    "\n",
    "    worker_node {\n",
    "      vm_size               = \"Standard_D12_V2\" # 4 cpu 28 ram\n",
    "      username              = \"azureuser\"\n",
    "      password              = \"Password123!\"\n",
    "      target_instance_count = 2\n",
    "    }\n",
    "\n",
    "    zookeeper_node {\n",
    "      vm_size  = \"Standard_A2_V2\"  # 2 cpu 4 ram\n",
    "      username = \"azureuser\"\n",
    "      password = \"Password123!s\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "output \"ssh_endpoint\" {\n",
    "    value = azurerm_hdinsight_hadoop_cluster.lsml_hc.ssh_endpoint\n",
    "}\n",
    "\n",
    "output \"https_endpoint\" {\n",
    "    value = azurerm_hdinsight_hadoop_cluster.lsml_hc.https_endpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing common.tf\n"
     ]
    }
   ],
   "source": [
    "%%writefile common.tf\n",
    "\n",
    "provider \"azurerm\" {\n",
    "  subscription_id = \"3750c13c-7871-4812-9fd5-94fb4ce86681\"\n",
    "  features {}\n",
    "}\n",
    "\n",
    "\n",
    "resource \"azurerm_resource_group\" \"lsml_rg\" {\n",
    "  name = \"lsml-resource-group\"\n",
    "  location = \"westus\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_virtual_network\" \"lsml_vn\" {\n",
    "  name = \"lsml-vitrual-network\"\n",
    "  resource_group_name = azurerm_resource_group.lsml_rg.name\n",
    "  location = azurerm_resource_group.lsml_rg.location\n",
    "  address_space = [\"10.0.0.0/16\"]   # Пул адресов внутри сети\n",
    "}\n",
    "\n",
    "resource \"azurerm_storage_account\" \"lsml_sa\" {\n",
    "  name                     = \"arinkalsmlstorage\"\n",
    "  resource_group_name      = azurerm_resource_group.lsml_rg.name\n",
    "  location                 = azurerm_resource_group.lsml_rg.location\n",
    "  account_tier             = \"Standard\"\n",
    "  account_replication_type = \"LRS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0m\u001b[1mInitializing the backend...\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1mInitializing provider plugins...\u001b[0m\n",
      "- Finding latest version of hashicorp/azurerm...\n",
      "- Installing hashicorp/azurerm v2.49.0...\n",
      "- Installed hashicorp/azurerm v2.49.0 (signed by HashiCorp)\n",
      "\n",
      "Terraform has created a lock file \u001b[1m.terraform.lock.hcl\u001b[0m to record the provider\n",
      "selections it made above. Include this file in your version control repository\n",
      "so that Terraform can guarantee to make the same selections by default when\n",
      "you run \"terraform init\" in the future.\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[32mTerraform has been successfully initialized!\u001b[0m\u001b[32m\u001b[0m\n",
      "\u001b[0m\u001b[32m\n",
      "You may now begin working with Terraform. Try running \"terraform plan\" to see\n",
      "any changes that are required for your infrastructure. All Terraform commands\n",
      "should now work.\n",
      "\n",
      "If you ever set or change modules or backend configuration for Terraform,\n",
      "rerun this command to reinitialize your working directory. If you forget, other\n",
      "commands will detect it and remind you to do so if necessary.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! terraform init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An execution plan has been generated and is shown below.\n",
      "Resource actions are indicated with the following symbols:\n",
      "  \u001b[32m+\u001b[0m create\n",
      "\u001b[0m\n",
      "Terraform will perform the following actions:\n",
      "\n",
      "\u001b[1m  # azurerm_hdinsight_hadoop_cluster.lsml_hc\u001b[0m will be created\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" {\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mcluster_version\u001b[0m\u001b[0m     = \"4.0\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhttps_endpoint\u001b[0m\u001b[0m      = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m                  = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mlocation\u001b[0m\u001b[0m            = \"westus\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m                = \"hw2-lsml-hdicluster\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mresource_group_name\u001b[0m\u001b[0m = \"lsml-resource-group\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_endpoint\u001b[0m\u001b[0m        = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mtier\u001b[0m\u001b[0m                = \"Standard\"\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mcomponent_version {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhadoop\u001b[0m\u001b[0m = \"3.1\"\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mgateway {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0menabled\u001b[0m\u001b[0m  = true\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\"\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mroles {\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mhead_node {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\"\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m  = \"A5\"\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mworker_node {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mmin_instance_count\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m              = (sensitive value)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mtarget_instance_count\u001b[0m\u001b[0m = 2\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m              = \"azureuser\"\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m               = \"Standard_D12_V2\"\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mzookeeper_node {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\"\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m  = \"Standard_A2_V2\"\n",
      "            }\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mstorage_account {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mis_default\u001b[0m\u001b[0m           = true\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_account_key\u001b[0m\u001b[0m  = (sensitive value)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_container_id\u001b[0m\u001b[0m = (known after apply)\n",
      "        }\n",
      "    }\n",
      "\n",
      "\u001b[1m  # azurerm_resource_group.lsml_rg\u001b[0m will be created\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_resource_group\" \"lsml_rg\" {\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m       = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mlocation\u001b[0m\u001b[0m = \"westus\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m     = \"lsml-resource-group\"\n",
      "    }\n",
      "\n",
      "\u001b[1m  # azurerm_storage_account.lsml_sa\u001b[0m will be created\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_storage_account\" \"lsml_sa\" {\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0maccess_tier\u001b[0m\u001b[0m                      = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0maccount_kind\u001b[0m\u001b[0m                     = \"StorageV2\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0maccount_replication_type\u001b[0m\u001b[0m         = \"LRS\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0maccount_tier\u001b[0m\u001b[0m                     = \"Standard\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallow_blob_public_access\u001b[0m\u001b[0m         = false\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0menable_https_traffic_only\u001b[0m\u001b[0m        = true\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m                               = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mis_hns_enabled\u001b[0m\u001b[0m                   = false\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mlarge_file_share_enabled\u001b[0m\u001b[0m         = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mlocation\u001b[0m\u001b[0m                         = \"westus\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mmin_tls_version\u001b[0m\u001b[0m                  = \"TLS1_0\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m                             = \"arinkalsmlstorage\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_access_key\u001b[0m\u001b[0m               = (sensitive value)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_blob_connection_string\u001b[0m\u001b[0m   = (sensitive value)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_blob_endpoint\u001b[0m\u001b[0m            = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_blob_host\u001b[0m\u001b[0m                = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_connection_string\u001b[0m\u001b[0m        = (sensitive value)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_dfs_endpoint\u001b[0m\u001b[0m             = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_dfs_host\u001b[0m\u001b[0m                 = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_file_endpoint\u001b[0m\u001b[0m            = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_file_host\u001b[0m\u001b[0m                = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_location\u001b[0m\u001b[0m                 = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_queue_endpoint\u001b[0m\u001b[0m           = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_queue_host\u001b[0m\u001b[0m               = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_table_endpoint\u001b[0m\u001b[0m           = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_table_host\u001b[0m\u001b[0m               = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_web_endpoint\u001b[0m\u001b[0m             = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprimary_web_host\u001b[0m\u001b[0m                 = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mresource_group_name\u001b[0m\u001b[0m              = \"lsml-resource-group\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_access_key\u001b[0m\u001b[0m             = (sensitive value)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_blob_connection_string\u001b[0m\u001b[0m = (sensitive value)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_blob_endpoint\u001b[0m\u001b[0m          = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_blob_host\u001b[0m\u001b[0m              = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_connection_string\u001b[0m\u001b[0m      = (sensitive value)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_dfs_endpoint\u001b[0m\u001b[0m           = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_dfs_host\u001b[0m\u001b[0m               = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_file_endpoint\u001b[0m\u001b[0m          = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_file_host\u001b[0m\u001b[0m              = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_location\u001b[0m\u001b[0m               = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_queue_endpoint\u001b[0m\u001b[0m         = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_queue_host\u001b[0m\u001b[0m             = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_table_endpoint\u001b[0m\u001b[0m         = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_table_host\u001b[0m\u001b[0m             = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_web_endpoint\u001b[0m\u001b[0m           = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msecondary_web_host\u001b[0m\u001b[0m               = (known after apply)\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mblob_properties {\n",
      "          \u001b[32m+\u001b[0m \u001b[0mcors_rule {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallowed_headers\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallowed_methods\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallowed_origins\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mexposed_headers\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mmax_age_in_seconds\u001b[0m\u001b[0m = (known after apply)\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mdelete_retention_policy {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mdays\u001b[0m\u001b[0m = (known after apply)\n",
      "            }\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0midentity {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mprincipal_id\u001b[0m\u001b[0m = (known after apply)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mtenant_id\u001b[0m\u001b[0m    = (known after apply)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mtype\u001b[0m\u001b[0m         = (known after apply)\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mnetwork_rules {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mbypass\u001b[0m\u001b[0m                     = (known after apply)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mdefault_action\u001b[0m\u001b[0m             = (known after apply)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mip_rules\u001b[0m\u001b[0m                   = (known after apply)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvirtual_network_subnet_ids\u001b[0m\u001b[0m = (known after apply)\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mqueue_properties {\n",
      "          \u001b[32m+\u001b[0m \u001b[0mcors_rule {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallowed_headers\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallowed_methods\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mallowed_origins\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mexposed_headers\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mmax_age_in_seconds\u001b[0m\u001b[0m = (known after apply)\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mhour_metrics {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0menabled\u001b[0m\u001b[0m               = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0minclude_apis\u001b[0m\u001b[0m          = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mretention_policy_days\u001b[0m\u001b[0m = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mversion\u001b[0m\u001b[0m               = (known after apply)\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mlogging {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mdelete\u001b[0m\u001b[0m                = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mread\u001b[0m\u001b[0m                  = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mretention_policy_days\u001b[0m\u001b[0m = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mversion\u001b[0m\u001b[0m               = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mwrite\u001b[0m\u001b[0m                 = (known after apply)\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mminute_metrics {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0menabled\u001b[0m\u001b[0m               = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0minclude_apis\u001b[0m\u001b[0m          = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mretention_policy_days\u001b[0m\u001b[0m = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mversion\u001b[0m\u001b[0m               = (known after apply)\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "\n",
      "\u001b[1m  # azurerm_storage_container.lsml_hadoop_sc\u001b[0m will be created\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_storage_container\" \"lsml_hadoop_sc\" {\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mcontainer_access_type\u001b[0m\u001b[0m   = \"private\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhas_immutability_policy\u001b[0m\u001b[0m = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhas_legal_hold\u001b[0m\u001b[0m          = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m                      = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mmetadata\u001b[0m\u001b[0m                = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m                    = \"hw2-lslm-storage-container\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mresource_manager_id\u001b[0m\u001b[0m     = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_account_name\u001b[0m\u001b[0m    = \"arinkalsmlstorage\"\n",
      "    }\n",
      "\n",
      "\u001b[1m  # azurerm_virtual_network.lsml_vn\u001b[0m will be created\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_virtual_network\" \"lsml_vn\" {\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0maddress_space\u001b[0m\u001b[0m         = [\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\"10.0.0.0/16\",\n",
      "        ]\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mguid\u001b[0m\u001b[0m                  = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m                    = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mlocation\u001b[0m\u001b[0m              = \"westus\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m                  = \"lsml-vitrual-network\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mresource_group_name\u001b[0m\u001b[0m   = \"lsml-resource-group\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0msubnet\u001b[0m\u001b[0m                = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_protection_enabled\u001b[0m\u001b[0m = false\n",
      "    }\n",
      "\n",
      "\u001b[0m\u001b[1mPlan:\u001b[0m 5 to add, 0 to change, 0 to destroy.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mChanges to Outputs:\u001b[0m\n",
      "  \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhttps_endpoint\u001b[0m\u001b[0m = (known after apply)\n",
      "  \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_endpoint\u001b[0m\u001b[0m   = (known after apply)\u001b[0m\n",
      "\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1m\"gateway.0.enabled\": [DEPRECATED] HDInsight doesn't support disabling gateway anymore\u001b[0m\n",
      "\n",
      "\u001b[0m  on hadoop.tf line 8, in resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\":\n",
      "   8: resource \"azurerm_hdinsight_hadoop_cluster\" \"lsml_hc\" \u001b[4m{\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mDo you want to perform these actions?\u001b[0m\n",
      "  Terraform will perform the actions described above.\n",
      "  Only 'yes' will be accepted to approve.\n",
      "\n",
      "  \u001b[1mEnter a value:\u001b[0m \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_resource_group.lsml_rg: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_resource_group.lsml_rg: Creation complete after 7s [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_virtual_network.lsml_vn: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_virtual_network.lsml_vn: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_virtual_network.lsml_vn: Creation complete after 18s [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Network/virtualNetworks/lsml-vitrual-network]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Creation complete after 45s [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Storage/storageAccounts/arinkalsmlstorage]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_container.lsml_hadoop_sc: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_container.lsml_hadoop_sc: Creation complete after 1s [id=https://arinkalsmlstorage.blob.core.windows.net/hw2-lslm-storage-container]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [1m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [1m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [1m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [1m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [1m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [1m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [2m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [2m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [2m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [2m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [2m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [2m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [3m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [3m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [3m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [3m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [3m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [3m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [4m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [4m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [4m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [4m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [4m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [4m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [5m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [5m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [5m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [5m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [5m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [5m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [6m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [6m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [6m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [6m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [6m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [6m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [7m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [7m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [7m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [7m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [7m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [7m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [8m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [8m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [8m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [8m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [8m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [8m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [9m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [9m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [9m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [9m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [9m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [9m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [10m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [11m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [11m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [11m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [11m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [11m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [11m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [12m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [12m10s elapsed]\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [12m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [12m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [12m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [12m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [13m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [13m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [13m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [13m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [13m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [13m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [14m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [14m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [14m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [14m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [14m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [14m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [15m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [15m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [15m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [15m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [15m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [15m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [16m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [16m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [16m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [16m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [16m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [16m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [17m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [17m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [17m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [17m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [17m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [17m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [18m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [18m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [18m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [18m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [18m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [18m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [19m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [19m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [19m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [19m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [19m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [19m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [20m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [21m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [21m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [21m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [21m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [21m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [21m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [22m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [22m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [22m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [22m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [22m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [22m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [23m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [23m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [23m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [23m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [23m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [23m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [24m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [24m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [24m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [24m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [24m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [24m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [25m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [25m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [25m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [25m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [25m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [25m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [26m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [26m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [26m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [26m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [26m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [26m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [27m1s elapsed]\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [27m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [27m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [27m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [27m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [27m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [28m1s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [28m11s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [28m21s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [28m31s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [28m41s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Still creating... [28m51s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_hadoop_cluster.lsml_hc: Creation complete after 28m56s [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.HDInsight/clusters/hw2-lsml-hdicluster]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Apply complete! Resources: 5 added, 0 changed, 0 destroyed.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Outputs:\n",
      "\n",
      "https_endpoint = \"hw2-lsml-hdicluster.azurehdinsight.net\"\n",
      "ssh_endpoint = \"hw2-lsml-hdicluster-ssh.azurehdinsight.net\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! echo \"yes\" | terraform apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https_endpoint = \"hw2-lsml-hdicluster.azurehdinsight.net\"\r\n",
      "ssh_endpoint = \"hw2-lsml-hdicluster-ssh.azurehdinsight.net\"\r\n"
     ]
    }
   ],
   "source": [
    "! terraform output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. [+5 баллов]** В `VisitsStream.tsv` лежит информация про пользователей, которые открывают сайт. Используя классический Hadoop MapReduce необходимо посчитать топ 10 пользователей с самыми длинными по времени сессиями и время этой самой длинной сессии (в секундах).\n",
    "\n",
    "Сессия определяется следующим образом - это окно времени, внутри которого временное расстояние от двух соседних посещений не более **15 минут**. \n",
    "\n",
    "Иными словами - если пользователь зашел на сайт в момент X и последнее предыдущее посещение сайта в момент Y было не позднее чем 15 минут назад, то сессия \"продлевается\" до текущего момента. Если же временное расстояние от X до Y более 15 минут, то считается, что предыдущая сессия закончилась в момент Y, а новая сессия началась в момент X.\n",
    "\n",
    "Сессия может длится 0 секунд, если пользователь сделал всего 1 запрос в течение 30 минутного окна (в середине этого окна). Считается, что в начале у пользователя нет открытой сессии и что сессия автоматически заканчивается, когда записей больше не осталось.\n",
    "\n",
    "Выводить нужно только уникальных пользователей и для каждого такого пользователя находить время самой длинной его сессии. \n",
    "\n",
    "При решении можно использовать произвольное количество MapReduce задач, но чем меньше, тем лучше. За излишне неоптимальное решение можно потерять балл. \n",
    "\n",
    "Полученный файл с топ 10 нужно будет выложить в облако, обеспечить публичный доступ до него и приложить к решению.\n",
    "\n",
    "В ноутбуке должны присутствовать ячейки с \n",
    "\n",
    "1) Всеми необходимыми скриптами для работы ваших MapReduce задач\n",
    "\n",
    "2) Командами запуска самих MapReduce задач\n",
    "\n",
    "3) Ссылкой на итоговый результат работы в вашем облаке. Ссылки должны быть рабочими до того момента, как вашу домашку не проверят.\n",
    "\n",
    "Пример итогового файла\n",
    "\n",
    "```bash\n",
    "1000094\t6852\n",
    "1000030\t4237\n",
    "1000003\t1932\n",
    "1000058\t1885\n",
    "100010\t1132\n",
    "1000012\t1086\n",
    "1000067\t657\n",
    "1000111\t244\n",
    "1000085\t197\n",
    "1000049\t131\n",
    "```\n",
    "\n",
    "**Важно** Задачу нужно решить именно на `Hadoop MapReduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing find_sessions.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def mapper():\n",
    "    for row in csv.reader(iter(sys.stdin.readline, ''), delimiter=\"\\t\"):\n",
    "        user_id, visit_datetime = row[0], row[3]\n",
    "        print(\"{}+{}\\t\".format(user_id, visit_datetime))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    datetime_fmt = '%Y-%m-%d %H:%M:%S.%f'\n",
    "    time_const = 15 * 60\n",
    "    key, _ = next(sys.stdin).split('\\t')\n",
    "    user_id, visit_datetime = key.split('+')\n",
    "    visit_datetime = datetime.strptime(visit_datetime.rstrip('\\n'), datetime_fmt)\n",
    "    max_session, curr_session = 0, 0\n",
    "    for line in sys.stdin:\n",
    "        curr_key, _ = line.split('\\t')\n",
    "        curr_user_id, curr_visit_datetime = curr_key.split('+')\n",
    "        curr_visit_datetime = datetime.strptime(curr_visit_datetime.rstrip('\\n'), datetime_fmt)\n",
    "        seconds_delta = (curr_visit_datetime - visit_datetime).total_seconds()\n",
    "        if curr_user_id != user_id:\n",
    "            print(\"{}\\t{}\".format(user_id, max(max_session, curr_session)))\n",
    "            max_session, curr_session = 0, 0\n",
    "        elif seconds_delta <= time_const:\n",
    "            curr_session += seconds_delta\n",
    "        else:\n",
    "            max_session = max(max_session, curr_session)\n",
    "            curr_session = 0\n",
    "        user_id = curr_user_id\n",
    "        visit_datetime = curr_visit_datetime\n",
    "    print(\"{}\\t{}\".format(user_id, max(max_session, curr_session)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /result || true\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"longest-sessions\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-files find_sessions.py \\\n",
    "-mapper \"python3 find_sessions.py map\" \\\n",
    "-reducer \"python3 find_sessions.py reduce\" \\\n",
    "-input /data/ \\\n",
    "-output /result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def _rewind_stream(stream):\n",
    "    for _ in stream:\n",
    "        pass\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in sys.stdin:\n",
    "        key, value = row.split('\\t')\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for _ in range(10):\n",
    "        key, _ = next(sys.stdin).split('\\t')\n",
    "        word, count = key.split(\"+\")\n",
    "        print(\"{}\\t{}\".format(word, count))\n",
    "    _rewind_stream(sys.stdin)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /top10/ || true\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"get_top_10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "g\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files get_top_10.py \\\n",
    "-mapper \"python get_top_10.py map\" \\\n",
    "-reducer \"python get_top_10.py reduce\" \\\n",
    "-input /result/ \\\n",
    "-output /top10/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на файл:\n",
    "https://arinkalsmlstorage.blob.core.windows.net/hw2-lslm-storage-container/top10/part-00000?sp=r&st=2021-03-01T12:57:18Z&se=2021-04-01T20:57:18Z&spr=https&sv=2020-02-10&sr=b&sig=cmpo7kaJZ%2BNHCBuNLYNmlDEXBvOHWhyLWphNm7qy06Q%3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. [+5 баллов]** В этой секции будет работать с большим количеством таблиц, которые есть в датасете. Подробное описание данных в этих таблицах и их взаимосвязей есть на странице Kaggle - https://www.kaggle.com/c/avito-context-ad-clicks/data\n",
    "\n",
    "Схема данных следующая \n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/4438/media/DB_schema.png\">\n",
    "\n",
    "Для данных нужно подсчитать некоторые статистики.\n",
    "\n",
    "**1. [1 балл]** Найти топ 10 самых популярных фильтров. Фильтры кодируются ключами в словаре SearchParams (именно ключи (числа), а не значения). Задача внезапно творческая и будут приниматься любые разумные подходы (aka костыли), которые решат задачу. Удачи :)\n",
    "\n",
    "**2. [1 балл]** Найдите топ 10 самых встречаемых слов в запросах, в которых пользователь кликнул по рекламе. Слова должны быть приведены к нижнеу регистру. \n",
    "\n",
    "Для примера\n",
    "\n",
    "```bash\n",
    "\"Купить стол\" -> Кликнул\n",
    "\"Ноутбук\" -> Кликнул\n",
    "\"Купить машину\" -> Кликнул\n",
    "\"Красивый стол\" -> Не кликнул\n",
    "\"Большой стол\" -> Кликнул\n",
    "\"Купить маску\" -> Кликнул\n",
    "```\n",
    "\n",
    "Топ слов (с указанием того, сколько оно встретилось)\n",
    "\n",
    "```bash\n",
    "купить - 3\n",
    "стол - 2\n",
    "ноутбук - 1\n",
    "большой - 1\n",
    "маску - 1\n",
    "машину - 1\n",
    "```\n",
    "\n",
    "**3. [1 балл]** Для каждого слова из заголовка объявления подсчитать его среднюю стоимость. Средняя стоимость слова - это среднее стоимости всех объявлений, где оно встретилось. Так например если слово появилось в заголовке рекламы с ценой A и в заголовке рекламы с ценой B, то его средняя стоимость - (A+B)/2 . Слова должны быть приведены к нижнему регистру.\n",
    "\n",
    "Учитывать необходимо только записи, где есть указаная стоимость и заголовок.\n",
    "\n",
    "В качестве ответа - топ 10 самых дорогих слов с указанием их средней стоимости.\n",
    "\n",
    "**4 [1 балл]** Найдите всех пользователей, которые заходили каждый день на протяжении всего времени измерений. В качестве ответа запишите одно число - количество этих пользвателей.\n",
    "\n",
    "**5 [1 балл]** Для каждого дня найдите количество уникальных пользователей, которые заходили на сайт в этот день. Выкиньте из рассмотрения всех пользователей, которые вы нашли в пункте 4 (то есть тех, которые заходили каждый день какого-либо месяца). \n",
    "\n",
    "В ответе укажите пары день-число уникальных пользователей в порядке убывания количества.\n",
    "\n",
    "**Важно!** Результаты каждого из 5 пунктов сохраните в виде файла в облачное хранилище. Ссылки на все 5 файлов должны быть указаны в работе. \n",
    "\n",
    "Итого, в ноутбуке должны присутствовать \n",
    "\n",
    "1) Ячейки с кодом на Spark\n",
    "\n",
    "2) Ссылки на все файлы в облаке\n",
    "\n",
    "**Важно** Задачу нужно решить именно на `Apache Spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing common.tf\n"
     ]
    }
   ],
   "source": [
    "%%writefile common.tf\n",
    "\n",
    "provider \"azurerm\" {\n",
    "  subscription_id = \"3750c13c-7871-4812-9fd5-94fb4ce86681\"\n",
    "  features {}\n",
    "}\n",
    "\n",
    "\n",
    "resource \"azurerm_resource_group\" \"lsml_rg\" {\n",
    "  name = \"lsml-resource-group\"\n",
    "  location = \"westus\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_virtual_network\" \"lsml_vn\" {\n",
    "  name = \"lsml-vitrual-network\"\n",
    "  resource_group_name = azurerm_resource_group.lsml_rg.name\n",
    "  location = azurerm_resource_group.lsml_rg.location\n",
    "  address_space = [\"10.0.0.0/16\"]   # Пул адресов внутри сети\n",
    "}\n",
    "\n",
    "resource \"azurerm_storage_account\" \"lsml_sa\" {\n",
    "  name                     = \"\"\n",
    "  resource_group_name      = azurerm_resource_group.lsml_rg.name\n",
    "  location                 = azurerm_resource_group.lsml_rg.location\n",
    "  account_tier             = \"Standard\"\n",
    "  account_replication_type = \"LRS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spark.tf\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark.tf\n",
    "\n",
    "resource \"azurerm_storage_container\" \"spark_sc\" {\n",
    "  name                  = \"hw2-spark-container\"\n",
    "  storage_account_name  = azurerm_storage_account.lsml_sa.name\n",
    "  container_access_type = \"private\"\n",
    "}\n",
    "\n",
    "resource \"azurerm_hdinsight_spark_cluster\" \"spark_cluster\" {\n",
    "  name                = \"hw2-sparkcluster\"\n",
    "  resource_group_name = azurerm_resource_group.lsml_rg.name\n",
    "  location            = azurerm_resource_group.lsml_rg.location\n",
    "  cluster_version     = \"4.0\"\n",
    "  tier                = \"Standard\"\n",
    "\n",
    "  component_version {\n",
    "    spark = \"2.4\"\n",
    "  }\n",
    "\n",
    "  gateway {\n",
    "    enabled  = true\n",
    "    username = \"azureuser\"\n",
    "    password = \"Password123!\"\n",
    "  }\n",
    "\n",
    "  storage_account {\n",
    "    storage_container_id = azurerm_storage_container.spark_sc.id\n",
    "    storage_account_key  = azurerm_storage_account.lsml_sa.primary_access_key\n",
    "    is_default           = true\n",
    "  }\n",
    "\n",
    "  roles {\n",
    "    head_node {\n",
    "      vm_size  = \"A6\"  # 2 cpu 14 ram\n",
    "      username = \"azureuser\"\n",
    "      password = \"Password123!\"\n",
    "    }\n",
    "\n",
    "    worker_node {\n",
    "      vm_size               = \"Standard_D12_V2\" # 4 cpu 28 ram\n",
    "      username              = \"azureuser\"\n",
    "      password              = \"Password123!\"\n",
    "      target_instance_count = 1\n",
    "    }\n",
    "\n",
    "    zookeeper_node {\n",
    "      vm_size  = \"Medium\"\n",
    "      username = \"azureuser\"\n",
    "      password = \"Password123!\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "output \"ssh_endpoint\" {\n",
    "    value = azurerm_hdinsight_spark_cluster.spark_cluster.ssh_endpoint\n",
    "}\n",
    "\n",
    "output \"https_endpoint\" {\n",
    "    value = azurerm_hdinsight_spark_cluster.spark_cluster.https_endpoint\n",
    "}\n",
    "\n",
    "output \"jupyter_endpoint\" {\n",
    "    value = \"${azurerm_hdinsight_spark_cluster.spark_cluster.https_endpoint}/jupyter\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0m\u001b[1mInitializing the backend...\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1mInitializing provider plugins...\u001b[0m\n",
      "- Finding latest version of hashicorp/azurerm...\n",
      "- Installing hashicorp/azurerm v2.50.0...\n",
      "- Installed hashicorp/azurerm v2.50.0 (signed by HashiCorp)\n",
      "\n",
      "Terraform has created a lock file \u001b[1m.terraform.lock.hcl\u001b[0m to record the provider\n",
      "selections it made above. Include this file in your version control repository\n",
      "so that Terraform can guarantee to make the same selections by default when\n",
      "you run \"terraform init\" in the future.\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[32mTerraform has been successfully initialized!\u001b[0m\u001b[32m\u001b[0m\n",
      "\u001b[0m\u001b[32m\n",
      "You may now begin working with Terraform. Try running \"terraform plan\" to see\n",
      "any changes that are required for your infrastructure. All Terraform commands\n",
      "should now work.\n",
      "\n",
      "If you ever set or change modules or backend configuration for Terraform,\n",
      "rerun this command to reinitialize your working directory. If you forget, other\n",
      "commands will detect it and remind you to do so if necessary.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! terraform init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_resource_group.lsml_rg: Importing from ID \"/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group\"...\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32mazurerm_resource_group.lsml_rg: Import prepared!\u001b[0m\n",
      "\u001b[0m\u001b[32m  Prepared azurerm_resource_group for import\u001b[0m\n",
      "\u001b[31m\n",
      "\u001b[1m\u001b[31mError: \u001b[0m\u001b[0m\u001b[1mResource already managed by Terraform\u001b[0m\n",
      "\n",
      "\u001b[0mTerraform is already managing a remote object for\n",
      "azurerm_resource_group.lsml_rg. To import to this address you must first\n",
      "remove the existing object from the state.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_virtual_network.lsml_vn: Importing from ID \"/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Network/virtualNetworks/lsml-vitrual-network\"...\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32mazurerm_virtual_network.lsml_vn: Import prepared!\u001b[0m\n",
      "\u001b[0m\u001b[32m  Prepared azurerm_virtual_network for import\u001b[0m\n",
      "\u001b[31m\n",
      "\u001b[1m\u001b[31mError: \u001b[0m\u001b[0m\u001b[1mResource already managed by Terraform\u001b[0m\n",
      "\n",
      "\u001b[0mTerraform is already managing a remote object for\n",
      "azurerm_virtual_network.lsml_vn. To import to this address you must first\n",
      "remove the existing object from the state.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Importing from ID \"/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Storage/storageAccounts/arinkalsmlstorage\"...\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32mazurerm_storage_account.lsml_sa: Import prepared!\u001b[0m\n",
      "\u001b[0m\u001b[32m  Prepared azurerm_storage_account for import\u001b[0m\n",
      "\u001b[31m\n",
      "\u001b[1m\u001b[31mError: \u001b[0m\u001b[0m\u001b[1mResource already managed by Terraform\u001b[0m\n",
      "\n",
      "\u001b[0mTerraform is already managing a remote object for\n",
      "azurerm_storage_account.lsml_sa. To import to this address you must first\n",
      "remove the existing object from the state.\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_container.spark_sc: Importing from ID \"https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container\"...\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32mazurerm_storage_container.spark_sc: Import prepared!\u001b[0m\n",
      "\u001b[0m\u001b[32m  Prepared azurerm_storage_container for import\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_container.spark_sc: Refreshing state... [id=https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container]\u001b[0m\n",
      "\u001b[0m\u001b[32m\n",
      "Import successful!\n",
      "\n",
      "The resources that were imported are shown above. These resources are now in\n",
      "your Terraform state and will henceforth be managed by Terraform.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!terraform import azurerm_resource_group.lsml_rg /subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group\n",
    "!terraform import azurerm_virtual_network.lsml_vn /subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Network/virtualNetworks/lsml-vitrual-network\n",
    "!terraform import azurerm_storage_account.lsml_sa /subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Storage/storageAccounts/arinkalsmlstorage\n",
    "!terraform import azurerm_storage_container.spark_sc https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_resource_group.lsml_rg: Refreshing state... [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_virtual_network.lsml_vn: Refreshing state... [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Network/virtualNetworks/lsml-vitrual-network]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_account.lsml_sa: Refreshing state... [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.Storage/storageAccounts/arinkalsmlstorage]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_storage_container.spark_sc: Refreshing state... [id=https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container]\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Refreshing state... [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.HDInsight/clusters/hw2-sparkcluster]\u001b[0m\n",
      "\n",
      "An execution plan has been generated and is shown below.\n",
      "Resource actions are indicated with the following symbols:\n",
      "  \u001b[32m+\u001b[0m create\n",
      "\u001b[0m\n",
      "Terraform will perform the following actions:\n",
      "\n",
      "\u001b[1m  # azurerm_hdinsight_spark_cluster.spark_cluster\u001b[0m will be created\u001b[0m\u001b[0m\n",
      "\u001b[0m  \u001b[32m+\u001b[0m\u001b[0m resource \"azurerm_hdinsight_spark_cluster\" \"spark_cluster\" {\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mcluster_version\u001b[0m\u001b[0m     = \"4.0\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mhttps_endpoint\u001b[0m\u001b[0m      = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m                  = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mlocation\u001b[0m\u001b[0m            = \"westus\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mname\u001b[0m\u001b[0m                = \"hw2-sparkcluster\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mresource_group_name\u001b[0m\u001b[0m = \"lsml-resource-group\"\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_endpoint\u001b[0m\u001b[0m        = (known after apply)\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mtier\u001b[0m\u001b[0m                = \"Standard\"\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mcomponent_version {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mspark\u001b[0m\u001b[0m = \"2.4\"\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mgateway {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0menabled\u001b[0m\u001b[0m  = true\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\"\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mroles {\n",
      "          \u001b[32m+\u001b[0m \u001b[0mhead_node {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\"\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m  = \"A6\"\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mworker_node {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mmin_instance_count\u001b[0m\u001b[0m    = (known after apply)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m              = (sensitive value)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mtarget_instance_count\u001b[0m\u001b[0m = 1\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m              = \"azureuser\"\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m               = \"Standard_D12_V2\"\n",
      "            }\n",
      "\n",
      "          \u001b[32m+\u001b[0m \u001b[0mzookeeper_node {\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mpassword\u001b[0m\u001b[0m = (sensitive value)\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0musername\u001b[0m\u001b[0m = \"azureuser\"\n",
      "              \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvm_size\u001b[0m\u001b[0m  = \"Medium\"\n",
      "            }\n",
      "        }\n",
      "\n",
      "      \u001b[32m+\u001b[0m \u001b[0mstorage_account {\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mis_default\u001b[0m\u001b[0m           = true\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_account_key\u001b[0m\u001b[0m  = (sensitive value)\n",
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mstorage_container_id\u001b[0m\u001b[0m = \"https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container\"\n",
      "        }\n",
      "    }\n",
      "\n",
      "\u001b[0m\u001b[1mPlan:\u001b[0m 1 to add, 0 to change, 0 to destroy.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mChanges to Outputs:\u001b[0m\n",
      "  \u001b[33m~\u001b[0m \u001b[0m\u001b[1m\u001b[0mhttps_endpoint\u001b[0m\u001b[0m   = \"hw2-sparkcluster.azurehdinsight.net\" \u001b[33m->\u001b[0m \u001b[0m(known after apply)\n",
      "  \u001b[33m~\u001b[0m \u001b[0m\u001b[1m\u001b[0mjupyter_endpoint\u001b[0m\u001b[0m = \"hw2-sparkcluster.azurehdinsight.net/jupyter\" \u001b[33m->\u001b[0m \u001b[0m(known after apply)\n",
      "  \u001b[33m~\u001b[0m \u001b[0m\u001b[1m\u001b[0mssh_endpoint\u001b[0m\u001b[0m     = \"hw2-sparkcluster-ssh.azurehdinsight.net\" \u001b[33m->\u001b[0m \u001b[0m(known after apply)\u001b[0m\n",
      "\n",
      "\u001b[33m\n",
      "\u001b[1m\u001b[33mWarning: \u001b[0m\u001b[0m\u001b[1m\"gateway.0.enabled\": [DEPRECATED] HDInsight doesn't support disabling gateway anymore\u001b[0m\n",
      "\n",
      "\u001b[0m  on spark.tf line 8, in resource \"azurerm_hdinsight_spark_cluster\" \"spark_cluster\":\n",
      "   8: resource \"azurerm_hdinsight_spark_cluster\" \"spark_cluster\" \u001b[4m{\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mDo you want to perform these actions?\u001b[0m\n",
      "  Terraform will perform the actions described above.\n",
      "  Only 'yes' will be accepted to approve.\n",
      "\n",
      "  \u001b[1mEnter a value:\u001b[0m \u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Creating...\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [1m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [1m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [1m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [1m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [1m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [1m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [2m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [2m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [2m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [2m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [2m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [2m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [3m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [3m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [3m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [3m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [3m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [3m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [4m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [4m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [4m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [4m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [4m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [4m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [5m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [5m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [5m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [5m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [5m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [5m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [6m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [6m10s elapsed]\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [6m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [6m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [6m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [6m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [7m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [7m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [7m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [7m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [7m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [7m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [8m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [8m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [8m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [8m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [8m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [8m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [9m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [9m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [9m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [9m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [9m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [9m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [10m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [11m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [11m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [11m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [11m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [11m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [11m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [12m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [12m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [12m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [12m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [12m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [12m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [13m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [13m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [13m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [13m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [13m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [13m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [14m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [14m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [14m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [14m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [14m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [14m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [15m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [15m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [15m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [15m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [15m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [15m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [16m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [16m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [16m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [16m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [16m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [16m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [17m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [17m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [17m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [17m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [17m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [17m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [18m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [18m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [18m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [18m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [18m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [18m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [19m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [19m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [19m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [19m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [19m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [19m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20m10s elapsed]\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [20m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [21m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [21m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [21m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [21m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [21m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [21m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [22m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [22m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [22m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [22m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [22m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [22m50s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [23m0s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [23m10s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [23m20s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [23m30s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Still creating... [23m40s elapsed]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1mazurerm_hdinsight_spark_cluster.spark_cluster: Creation complete after 23m48s [id=/subscriptions/3750c13c-7871-4812-9fd5-94fb4ce86681/resourceGroups/lsml-resource-group/providers/Microsoft.HDInsight/clusters/hw2-sparkcluster]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Outputs:\n",
      "\n",
      "https_endpoint = \"hw2-sparkcluster.azurehdinsight.net\"\n",
      "jupyter_endpoint = \"hw2-sparkcluster.azurehdinsight.net/jupyter\"\n",
      "ssh_endpoint = \"hw2-sparkcluster-ssh.azurehdinsight.net\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! echo \"yes\" | terraform apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотела попробовать и работу с rdd и .map(), .reduce(), так и со spark sql, поэтому первое сделала одним способом, остальное другим.\n",
    "\n",
    "Еще наткнулась на такое, что в sql where a not in b, работает как-то подозрительно (как будто не все исключает или вообще не исключает) поэтому заменила на left join и where b is null, но не оч эстетичный вариант по мне (это в номере 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "sp = spark.sparkContext\n",
    "data = sp.textFile('/data/SearchInfo.tsv')\n",
    "\n",
    "\n",
    "def get_filter(raw_string):\n",
    "    parsed_line = next(csv.reader([raw_string], delimiter='\\t'))\n",
    "    search_params = parsed_line[-1]\n",
    "    if search_params not in ['', 'SearchParams']:\n",
    "        search_params = search_params[1:-1]\n",
    "        search_params = re.sub(r\"\\{[^{}]*\\}\", \"0\", search_params)\n",
    "        search_params = re.sub(r\"\\[[^\\[\\]]*\\]\", \"0\", search_params)\n",
    "        return list(ast.literal_eval(\"{\" + search_params + \"}\"))\n",
    "    return []\n",
    "\n",
    "\n",
    "(\n",
    "    data\n",
    "    .flatMap(get_filter)\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: (x[1], x[0]))\n",
    "    .sortByKey(ascending=False)\n",
    "    .map(lambda x: x[0])\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda x: x[1] < 10)\n",
    "    .saveAsTextFile('/top10_filters')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка на файл](https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container/top10_filters/final.txt?sp=r&st=2021-03-07T16:19:23Z&se=2021-03-08T00:19:23Z&spr=https&sv=2020-02-10&sr=b&sig=CDdNEqHU3F%2F5wzywa%2BRNCVGa7exuHamnJRTVIdYXYSA%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "sp = spark.sparkContext \n",
    "se = SparkSession(sc)\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"delimiter\",\"\\t\").load('/data/SearchInfo.tsv', header=True)\n",
    "df.registerTempTable('SearchInfo')\n",
    "df_stream = spark.read.format(\"csv\").option(\"delimiter\",\"\\t\").load('/data/trainSearchStream.tsv', header=True)\n",
    "df_stream.registerTempTable('SearchStream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = spark.sql(\"\"\"\n",
    "    WITH get_words AS (\n",
    "        WITH search_queries AS (\n",
    "            SELECT SearchQuery, SearchId\n",
    "            FROM SearchInfo\n",
    "            WHERE SearchQuery is not NULL\n",
    "        )\n",
    "        SELECT explode(split(SearchQuery, ' ')) AS word\n",
    "        FROM search_queries AS sq\n",
    "            JOIN SearchStream AS ss on ss.SearchId = sq.SearchId\n",
    "            WHERE IsClick = 1\n",
    "    )\n",
    "    SELECT LOWER(word) as word, count(*) AS num_occ\n",
    "    FROM get_words\n",
    "    GROUP BY LOWER(word)\n",
    "    ORDER BY num_occ DESC\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r /top10_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res.write.csv(\"/top10_queries/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка на файл](https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container/top10_queries/part-00000-b1eacec4-7bf7-464e-94cc-0dd508308aaf-c000.csv?sp=r&st=2021-03-07T16:24:16Z&se=2021-04-07T00:24:16Z&spr=https&sv=2020-02-10&sr=b&sig=kFSKH1DQ6bN3ddnn1ejVQelFmSphYbNq3EbJeQttkLA%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ads = spark.read.format(\"csv\").option(\"delimiter\",\"\\t\").load('/data/AdsInfo.tsv', header=True)\n",
    "df_ads.registerTempTable('AdsInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = spark.sql(\n",
    "    \"\"\"\n",
    "    with word_and_price as (\n",
    "        select explode(split(Title, ' ')) as word, Price as price\n",
    "        from AdsInfo\n",
    "        where Title is not null and Price is not null\n",
    "    )\n",
    "    select lower(word), avg(price) as avg_price\n",
    "    from word_and_price\n",
    "    group by lower(word)\n",
    "    order by avg_price desc\n",
    "    limit 10\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res.write.csv(\"/top10_words/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка на файл](https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container/top10_words/part-00000-280d008d-569c-4167-83c6-8229d8ae84e8-c000.csv?sp=r&st=2021-03-07T16:29:35Z&se=2021-04-07T00:29:35Z&spr=https&sv=2020-02-10&sr=b&sig=6R9EqTb3rDl%2BQLGwZWCWkYrahyZoxzYVtizHnnaeeJQ%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь я решила использавать VisitsStream.tsv и SearchInfo.tsv, потому что по идее пользователи могли смотреть объявление без поскового запроса, так и искать, но никуда не переходить. Разбила запрос на подзапросы для поиска таких пользователей (чтобы переиспользовать в 5) и непосредственный подсчет их количества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_visits_stream = spark.read.format(\"csv\").option(\"delimiter\",\"\\t\").load('/data/VisitsStream.tsv', header=True)\n",
    "df_visits_stream.registerTempTable('VisitsStream')\n",
    "df = spark.read.format(\"csv\").option(\"delimiter\",\"\\t\").load('/data/SearchInfo.tsv', header=True)\n",
    "df.registerTempTable('SearchInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    with all_visit_dates as (\n",
    "        with concat_visits as (\n",
    "            select userID as user, date(SearchDate) as visit_date\n",
    "            from SearchInfo\n",
    "\n",
    "            union \n",
    "\n",
    "            select userID as user, date(ViewDate) as visit_date\n",
    "            from VisitsStream\n",
    "        )\n",
    "        select user, visit_date, (select count(distinct(visit_date)) from concat_visits) as all_days\n",
    "        from concat_visits\n",
    "    )\n",
    "    select user\n",
    "    from all_visit_dates\n",
    "    group by user\n",
    "    having count(distinct(visit_date)) = first(all_days)\n",
    "    \"\"\"\n",
    ").registerTempTable('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select count(*)\n",
    "    from users\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir /task_4/\n",
    "echo '1068' > part00000.txt\n",
    "hdfs dfs -put part00000.txt /task_4/part00000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дней всего получилось 26, юзеров 1068, выходом одно число, поэтому просто записала его через echo\n",
    "\n",
    "[Ссылка на файл](https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container/task_4/part00000.txt?sp=r&st=2021-03-07T16:05:10Z&se=2021-04-07T00:05:10Z&spr=https&sv=2020-02-10&sr=b&sig=Ey5Ow3UAJfu7uIqQr2Okb8dZ0HT6q6AH%2FqOH42X5Jdc%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь согласна, что можно было просто вычесть количество пользователей из задания 4, но мне захотелось более общее решение, с исключением этих юзеров, чтобы условно получить не количество а список пользователей в каждый день получалось небольшим преобразованием кода. Но если это изолированная задачка на каждый день, а не условный EDA, то лучше вычитать константу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    with all_visit_dates as (\n",
    "        with concat_visits as (\n",
    "            select userID as user, date(SearchDate) as visit_date\n",
    "            from SearchInfo\n",
    "\n",
    "            union \n",
    "\n",
    "            select userID as user, date(ViewDate) as visit_date\n",
    "            from VisitsStream\n",
    "        )\n",
    "        select user, visit_date, (select count(distinct(visit_date)) from concat_visits) as all_days\n",
    "        from concat_visits\n",
    "    )\n",
    "    select user\n",
    "    from all_visit_dates\n",
    "    group by user\n",
    "    having count(distinct(visit_date)) = first(all_days)\n",
    "    \"\"\"\n",
    ").registerTempTable('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -getmerge /daily_visitors/*part* /var/lib/jupyter/task5.txt\n",
    "hdfs dfs -put /var/lib/jupyter/task5.txt /daily_visitors/final.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ссылка на файл](https://arinkalsmlstorage.blob.core.windows.net/hw2-spark-container/daily_visitors/final.txt?sp=r&st=2021-03-07T16:12:35Z&se=2021-04-08T00:12:35Z&spr=https&sv=2020-02-10&sr=b&sig=%2BXrYIF17VfoSQJKvB0VZTanohm7G5k2naA9mjmKp4KM%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно!** Следите за объемом потребляемой памяти! За решения, которые работают не оптимально по памяти, можно терять баллы. Понятное дело, что на текущих объемах скорее всего сработает примерно любое решение, но это не повод плохо писать алгоритм.\n",
    "\n",
    "Если в вашем алгоритме есть спорный момент в отношении использования памяти, но вы сделали это намерено - напишите явно в комментарии, что это осознаное решение, которое вы приняли по такой-то причине. Например вы могли запустить отдельную MR\\Spark задачу, которая бы показала, что во всем датасете определенного типа данных не более чем `M`, а значит мы не упремся в ограничения по памяти и вполне уместно использовать для его обработки именно такой подход.\n",
    "\n",
    "**Важно!** Можно как перезалить результат работы в бакет с публичным доступом, так и предоставить точных доступ до результата работы в HDFS (точнее в соответствующем бакете в WASB). Как это сделать, написано в дополнительной памятке к этой домашней работе.\n",
    "\n",
    "Если результат работы вашего алгоритма \"размазался\" по нескольким партам, то нужно дополнительно склеить их в один файл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Бонус [+5 дополнительных баллов]** Это задание творческое и не обязательное. В частности это означает, что у меня не будет к нему авторского решения и проверяться оно будет исходя не из формальных критериев, а из здравого смысла.\n",
    "\n",
    "Прочтите целиком, что вообще требуется предсказать в этой задаче на Kaggle от Avito - https://www.kaggle.com/c/avito-context-ad-clicks/overview . \n",
    "\n",
    "Придумайте признаки, составьте датасет и обучите логистическую регрессию для решения поставленной задачи. Все это должно быть на чистом спарк \"from scratch\". \n",
    "\n",
    "Пункты, которые повышают вероятность получения допбаллов за это задание:\n",
    "\n",
    "* Составлены адекватные признаки\n",
    "\n",
    "* Эффективно написан алгоритм обучения\n",
    "\n",
    "* Видно, что лосс падает\n",
    "\n",
    "* Есть разделение на обучающую и тестовую выборку (или использована та, что прилагается к задаче на Kaggle)\n",
    "\n",
    "* Подсчитана метрика качества"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
